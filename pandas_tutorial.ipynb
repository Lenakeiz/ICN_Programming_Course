{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICN Programming Course\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"500\" alt=\"image\" src=\"https://github.com/Lenakeiz/ICN_Programming_Course/blob/main/Images/cog_neuro_logo_blue_png_0.png?raw=true\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## Pandas\n",
    "In this part of the lesson we are going to present one of the tools you will most likely use for .\n",
    "\n",
    "[Pandas library](https://pandas.pydata.org/) defines and makes use of a new _data structure_, _i.e._ the `DataFrame`. \n",
    "Actually pandas define more than just a data structure, for instance we will make use of `Series` and examine the difference with dataframes.\n",
    "\n",
    "### Advantages of pandas\n",
    "\n",
    "Data scientists use Pandas for its following advantages:\n",
    "\n",
    "* Easily handles missing data;\n",
    "* It provides an efficient way to slice the data;\n",
    "* It provides a flexible way to merge, concatenate or reshape the data;\n",
    "* It includes a powerful data casting tool to work with;\n",
    "* It wraps data visualisation libraries in order to quickly plotting analysis results.\n",
    "\n",
    "Tthe main disadvantage of pandas is that it is relegated to manipulate dataframes whose dimension is strictly lower than memory. \n",
    "For bigger-than-memory datasets we need other libraries (`dask`, `duckdb`, `pyspark`, etc.). However, this is far and beyond the scope of this course, so we will focus on manageable dataframes for now (that with the modern computers memories can be quite huge in any case).\n",
    "\n",
    "### Dataframes\n",
    "\n",
    "A `DataFrame`, roughly speaking, is a table. As any other type in python, it is defined as a class, with its attributes and methods. \n",
    "You can check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to see the class APIs and see it s capability\n",
    "\n",
    "More formally, it is a rank-$2$ array, with axes labelled as _rows_ and _columns_.\n",
    "It is the basic object in pandas and a really common way to load data in memory in order to operate on them.\n",
    "\n",
    "Now the question you are all wondering: how to _create a dataframe_. There are several ways, by tuples, by lists, by numpy arrays or even by dictionaries. \n",
    "As a first instance, let's consider a list of names corresponding to people and their age, you can create a data frame in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pd is a standard alias for pandas library.\n",
    "\n",
    "# List of lists made by [str, int]\n",
    "lst = [['mario', 25], ['billy', 30], ['lakitu', 26], ['bowser', 22]]\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['Name', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict made by {str: int}\n",
    "data_dict = {'mario': 25, 'billy': 30, 'lakitu': 26, 'bowser': 22}\n",
    "\n",
    "df = pd.DataFrame(data=data_dict.items(), columns=['Name', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from csv file\n",
    "df = pd.read_csv('datasets/people.csv', header=None, names=['Name', 'Age'])\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from a numpy array\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([['mario', 25], ['billy', 30], ['lakitu', 26], ['bowser', 22]])\n",
    "df = pd.DataFrame(arr, columns=['Name', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have the exact same object.\n",
    "Once data are organised in the dataframe, no matter how we imported them, they are stored in that object that has always the same methods and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "A `Series` is a one-dimensional data structure. It can have any data structure like integer, float, and string, or even composite ones like lists, dictionaries, etc. \n",
    "\n",
    "It is useful when you want to perform computation or return a one-dimensional array.\n",
    "A series, by definition, cannot have multiple columns.\n",
    "For the latter case, use the data frame structure, which indeed can be considered as made up by series.\n",
    "\n",
    "Series has one parameters, the data, that can be a list, a dictionary, or a scalar value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1., 2., 3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from files\n",
    "\n",
    "Data can be loaded in a DataFrame from different data format, like csv, xlx, json, etc.\n",
    "\n",
    "The most common file type is the csv (comma separated values).\n",
    "You can load them into a `Dataframe` using `read_csv` method, eventually specifying also the type of delimeter used for separating the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel = pd.read_csv('datasets/travel_blog_data.csv', delimiter=';')\n",
    "df_travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset holds the user activity data from a travel blog. \n",
    "\n",
    "It is noteworthy a default behaviour in pandas `read_csv`.\n",
    "The csv file do not have a header row, therefore pandas used the first row of data as header; in order to set the name of the columns you can use the `name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel = pd.read_csv('datasets/travel_blog_data.csv', delimiter=';',\n",
    "                 names=['timestamp', 'event', 'country', 'user_id', 'source', 'topic'],\n",
    "                 parse_dates=True)\n",
    "df_travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it might be handy not to print the whole dataframe and flood your screen with data. \n",
    "When a few lines is enough, you can print only the first $n$ lines – by typing:\n",
    "\n",
    "```python\n",
    "df_travel.head(n)\n",
    "```\n",
    "\n",
    "If you leave the $n$ parameter blank, the method takes the default value, that is $5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By symmetry, you can imagine what the `tail` method returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also need a random sampling of $k$ lines out of the dataframe, this can be achieved by the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other two dataframe methods that are very useful in analysing data are `describe` and `info`.\n",
    "\n",
    "The `describe` method allows to get some statistical information about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_travel.describe()))\n",
    "df_travel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the result is again a dataframe whose index is a list of statistical properties, and as columns the values of indexed properties for the first available numerical column (in our case `user_id`).\n",
    "\n",
    "As one can read in the [docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) the method returns a Summary statistics of the Series or Dataframe provided.\n",
    "\n",
    "Again, from the official documentation \n",
    "\n",
    "> For numeric data, the result’s index will include `count`, `mean`, `std`, `min`, `max` as well as lower, $50$ and upper percentiles. By default the lower percentile is $25$ and the upper percentile is $75$. The $50$ percentile is the same as the median.\n",
    ">\n",
    "> For object data (e.g. strings or timestamps), the result’s index will include `count`, `unique`, `top`, and `freq`. The `top` is the most common value. The `freq` is the most common value’s frequency. `Timestamps` also include the first and last items.\n",
    ">\n",
    "> If multiple object values have the highest count, then the `count` and `top` results will be arbitrarily chosen from among those with the highest count.\n",
    ">\n",
    "> _For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type._\n",
    ">\n",
    "> The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.\n",
    "\n",
    "We can however try to override the default behaaviour and we can do so by setting `include='all'` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, we also have `info` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_info  = df.info()\n",
    "print(type(dataframe_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method actually returns a `NoneType`. But in the execution _prints_ on screen some information about the dataframe. \n",
    "It is less informative (almost not informative at all) from the statistics point of view, but it tells us some numerical property of the dataframe, indeed this method prints information about the DataFrame including the index `dtype` and columns, non-null values and memory usage.\n",
    "\n",
    "Interesting enough the first colum however should be able to be parsed into an object more easy to read. \n",
    "In particular, since the column clearly represents a date we can transform it into an object that is more recognisibile.\n",
    "We can parse it into `datetime` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dadtaframe function to_datetime() to convert the timestamp column to datetime\n",
    "# Saving the output directly to the timestamp column by replacing it\n",
    "df_travel['timestamp'] = pd.to_datetime(df_travel['timestamp'])\n",
    "df_travel.info()\n",
    "\n",
    "# Extracting date and time from the first row\n",
    "first_row_timestamp = df_travel.loc[0, 'timestamp']\n",
    "extracted_date = first_row_timestamp.date()\n",
    "extracted_time = first_row_timestamp.time()\n",
    "\n",
    "print(\"Date from the first row:\", extracted_date)\n",
    "print(\"Time from the first row:\", extracted_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last descriptive method we present here is the `corr` one. \n",
    "By using such a method we can generate the relationship between each numerical variables.\n",
    "This function will throw an error if you load a dataset that does not contain numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = pd.read_csv('datasets/memory_recall_test.csv', delimiter=',')\n",
    "df_memory.head()\n",
    "\n",
    "correlation_matrix = df_memory.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by selecting columns\n",
    "\n",
    "Sometimes you will need to only work with specific columns from a dataset.\n",
    "You can do so by using the two following and equivalent syntaxes.\n",
    "The second syntax is less flexible as it required the column names to be in a certain way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way, the square bracket notation\n",
    "df_memory['Age (Years)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second way, the point notation\n",
    "# the column name must not contain spaces, special characters, or starts with a number\n",
    "# if you want to use dot notation, you would need to rename the column to remove spaces and special characters.\n",
    "# for example:\n",
    "df_memory.rename(columns={'Age (Years)': 'Age_years'}, inplace=True)\n",
    "\n",
    "# the inplace parameter is used to modify the dataframe in place, without creating a new dataframe object otherwise you would need to assign the output to a new variable\n",
    "# new_df_memory = df_memory.rename(columns={'Age (Years)': 'Age_years'})\n",
    "df_memory.Age_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ Both of the previous syntaxes return a <em>Series</em> rather than a <em>Dataframe</em>\n",
    "</div>\n",
    "\n",
    "If you want a dataframe, you need to slightly change the previous commands in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard way, no one uses that.\n",
    "pd.DataFrame(df_memory.Age_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy way, it generalises easily to the multi-column case.\n",
    "df_memory[['Age_years']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about multi-column filter?\n",
    "\n",
    "As the previous cell might suggest, you only need to pass a list of columns.\n",
    "\n",
    "Note the double bracket `[[]]`, you can consider `[]` as a _filter_ operator, whose argument is the list of columns.\n",
    "Recall that a `Series` admits only one column, hence the result of this operation cannot be other than a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel[['user_id','country']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of names changes the order in the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel[['country', 'user_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rows on values\n",
    "\n",
    "There is complementary way of filtering a dataframe, on rows value. Hence, we can reduce the number of records in the dataframe based on some condition.\n",
    "\n",
    "Let's use the imnported travel dataframe, and for instance, you want to see the entries corresponding to the users who came from the \"SEO\" source. In this case you have to filter for the \"SEO\" value in the \"source\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel[df_travel.source == 'SEO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand the command above, let's focus on how pandas interpret the filtering procedure.\n",
    "\n",
    "**Step 1**: First, between the bracket frames `[]` it evaluates every line: is the `df.source` column’s value `'SEO'` or not? The results are boolean values (True or False), better a `Series` of boolean values. \n",
    "Indeed, we have seen how `df.source` is a series, a comparison with a value (through the binary operator `==`) will produce a truth-value object of the same type of `df.source` hence a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the dtype attribute\n",
    "df_travel.source == 'SEO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 2**: The previous boolean series is what is called a _mask_.\n",
    "If we filter through a mask, the filtered dataframes returns every row where the mask is `True` and drops any row where it is `False`.\n",
    "This is equivalent to the logical indexing in `Matlab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A less concise, but maybe clearer notation\n",
    "mask_seo = (df_travel.source == 'SEO') # Boolean series\n",
    "df_travel[mask_seo] # Masks away the rows corresponding to \"False\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious now that you can combine more conditions to end up into a boolean mask and apply even complicated filter.\n",
    "\n",
    "_Example_: We want to filter the dataframe to get all the users coming from a \"SEO\" source, with topic related to \"Asia\" and with a timestamp between 23.00 and 23.30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_mask = ((df_travel.source == 'SEO') & (df_travel.topic == 'Asia') & (df_travel.timestamp >= '2018-01-01 23:00:00') & (df_travel.timestamp <= '2018-01-01 23:30:00'))\n",
    "df_travel[bool_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new columns\n",
    "\n",
    "Creating a new column is easy, you simply _declare_ the new column name as `df[\"new_col\"] =  new_col` where `new_col` is a pandas Series.\n",
    "The new column will be filled with `NaN`` (Not a Number, which is Pandas' standard missing data marker) for all rows where the Series does not have a corresponding index.\n",
    "\n",
    "You can also calculate the new column entries by operating on existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column as the string concatenation of \"country\" and \"topic\"\n",
    "df_travel[\"contry_code_per_continent\"] = df_travel.country + \"||\" + df_travel.topic\n",
    "df_travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a further column made by the 2nd letter of the string contained in the \"event\" column\n",
    "df_travel[\"part_string\"] = df_travel.event.str[1]\n",
    "df_travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential filtering and masks \n",
    "\n",
    "Filter and mask operations can be used sequentially, one after the other.\n",
    "\n",
    "It is very important to understand that pandas’s logic is linear. \n",
    "So if you apply a function, you can always apply another one on the result. \n",
    "In this case, the input of the latter function will always be the output of the previous one. \n",
    "This will have a very nice consequence in a construct called `pipe` (see later for details).\n",
    "\n",
    "As you have already seen, for instance, we can combine two selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executed head() first then apply the column selection\n",
    "df_travel.head()[['country', 'user_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line first selects the first $5$ rows of our dataframe. And then it takes only the ‘country’ and the ‘user_id’ columns.\n",
    "\n",
    "As these operations are commutative, you would not be surprised by the fact that one can get the same result with the reversed chain of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel[['country', 'user_id']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, you select the columns first, then take the first five rows. \n",
    "The result is the same – just the order of the functions (and the execution) is different.\n",
    "\n",
    "_Try to think which one is better in terms of computational time._\n",
    "\n",
    "We can check it by running some very simple test using the Python in-built package time\n",
    "\n",
    "There a lot of interesting ways of selecting columns out of a dataframe. I suggest this [nice post](https://towardsdatascience.com/interesting-ways-to-select-pandas-dataframe-columns-b29b82bbfb33#:~:text=Selecting%20columns%20based%20on%20their,Returns%20a%20pandas%20series.&text=Passing%20a%20list%20in%20the,columns%20at%20the%20same%20time.) in order to look at some non-standard examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "df_large_dataset = pd.read_csv('./datasets/large_dataset.csv', delimiter=',')\n",
    "print(df_large_dataset.shape)\n",
    "# Initialize total time counters\n",
    "total_time_first_method = 0\n",
    "total_time_second_method = 0\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "for _ in range (iterations):\n",
    "    # Measure time for df_travel[['country', 'user_id']].head()\n",
    "    start_time = time.time()\n",
    "    df_large_dataset[['country', 'user_id']].head(100)\n",
    "    end_time = time.time()\n",
    "    total_time_first_method += end_time - start_time\n",
    "\n",
    "    # Measure time for df_travel.head()[['country', 'user_id']]\n",
    "    start_time = time.time()\n",
    "    df_large_dataset.head(100)[['country', 'user_id']]\n",
    "    end_time = time.time()\n",
    "    total_time_second_method += end_time - start_time\n",
    "\n",
    "# Compute average time for each method\n",
    "time_first_method = total_time_first_method / iterations\n",
    "time_second_method = total_time_second_method / iterations\n",
    "\n",
    "print(f\"Time for first method: {time_first_method:6f} seconds\")\n",
    "print(f\"Time for second method: {time_second_method:6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "In Python, particularly in the context of data analysis with libraries like Pandas, \"aggregation\" refers to the process of combining multiple data points into a single value based on a specified criterion or operation.\n",
    "This process is crucial in data analysis for summarizing data, extracting insights, and simplifying complex datasets.\n",
    "\n",
    "Let's consider a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>water_need</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elephant</td>\n",
       "      <td>1001</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elephant</td>\n",
       "      <td>1002</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elephant</td>\n",
       "      <td>1003</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tiger</td>\n",
       "      <td>1004</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiger</td>\n",
       "      <td>1005</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tiger</td>\n",
       "      <td>1006</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tiger</td>\n",
       "      <td>1007</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tiger</td>\n",
       "      <td>1008</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1009</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1010</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1011</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1012</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1013</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1014</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1015</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lion</td>\n",
       "      <td>1016</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lion</td>\n",
       "      <td>1017</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lion</td>\n",
       "      <td>1018</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lion</td>\n",
       "      <td>1019</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kangaroo</td>\n",
       "      <td>1020</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kangaroo</td>\n",
       "      <td>1021</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>kangaroo</td>\n",
       "      <td>1022</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      animal  uniq_id  water_need\n",
       "0   elephant     1001         500\n",
       "1   elephant     1002         600\n",
       "2   elephant     1003         550\n",
       "3      tiger     1004         300\n",
       "4      tiger     1005         320\n",
       "5      tiger     1006         330\n",
       "6      tiger     1007         290\n",
       "7      tiger     1008         310\n",
       "8      zebra     1009         200\n",
       "9      zebra     1010         220\n",
       "10     zebra     1011         240\n",
       "11     zebra     1012         230\n",
       "12     zebra     1013         220\n",
       "13     zebra     1014         100\n",
       "14     zebra     1015          80\n",
       "15      lion     1016         420\n",
       "16      lion     1017         600\n",
       "17      lion     1018         500\n",
       "18      lion     1019         390\n",
       "19  kangaroo     1020         410\n",
       "20  kangaroo     1021         430\n",
       "21  kangaroo     1022         410"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoo = pd.read_csv(\"datasets/zoo.csv\")\n",
    "zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of the animals is as easy as applying a `count` function on the zoo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can observe, the `count()` method counts the number of values in each column. \n",
    "In the case of the zoo dataset, there were $3$ columns, and each of them had $22$ values in it.\n",
    "\n",
    "If you want to make your output clearer, you can select the animal column first by using one of the selection operators from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo[['animal']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in this particular case, the result could be even nicer for Series, in fact to show just the number and not the column name nor the type, one can use the following syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.animal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as before, one can notice how pandas applied functions sequentially. \n",
    "First, `zoo.animal` is a Series, then calculated the count of it. \n",
    "\n",
    "Indeed an equivalent, even if less compact notation, for this is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_series = zoo.animal\n",
    "\n",
    "animal_series.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same logic, one can sum all the values inside a column by the `sum` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.water_need.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there are a lot of methods and one should know in advance what they are and how they works.\n",
    "\n",
    "In order to get such information, the royal road is to read the documentation. However, a convenient way to get a list of all available methods of a class instance is the function `dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_TO_AXIS_NUMBER',\n",
       " '_HANDLED_TYPES',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_ufunc__',\n",
       " '__arrow_c_stream__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__dataframe__',\n",
       " '__dataframe_consortium_standard__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pandas_priority__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_accum_func',\n",
       " '_agg_examples_doc',\n",
       " '_agg_see_also_doc',\n",
       " '_align_for_op',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_append',\n",
       " '_arith_method',\n",
       " '_arith_method_with_reindex',\n",
       " '_as_manager',\n",
       " '_attrs',\n",
       " '_box_col_values',\n",
       " '_can_fast_transpose',\n",
       " '_check_inplace_and_allows_duplicate_labels',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_label_or_level_ambiguity',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_cmp_method',\n",
       " '_combine_frame',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_result',\n",
       " '_constructor',\n",
       " '_constructor_from_mgr',\n",
       " '_constructor_sliced',\n",
       " '_constructor_sliced_from_mgr',\n",
       " '_create_data_for_split_and_tight_to_dict',\n",
       " '_data',\n",
       " '_deprecate_downcast',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_dispatch_frame_op',\n",
       " '_drop_axis',\n",
       " '_drop_labels_or_levels',\n",
       " '_ensure_valid_index',\n",
       " '_find_valid_index',\n",
       " '_flags',\n",
       " '_flex_arith_method',\n",
       " '_flex_cmp_method',\n",
       " '_from_arrays',\n",
       " '_from_mgr',\n",
       " '_get_agg_axis',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cleaned_column_resolvers',\n",
       " '_get_column_array',\n",
       " '_get_index_resolvers',\n",
       " '_get_item_cache',\n",
       " '_get_label_or_level_values',\n",
       " '_get_numeric_data',\n",
       " '_get_value',\n",
       " '_get_values_for_csv',\n",
       " '_getitem_bool_array',\n",
       " '_getitem_multilevel',\n",
       " '_getitem_nocopy',\n",
       " '_getitem_slice',\n",
       " '_gotitem',\n",
       " '_hidden_attrs',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_info_repr',\n",
       " '_init_mgr',\n",
       " '_inplace_method',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_copy',\n",
       " '_is_homogeneous_type',\n",
       " '_is_label_or_level_reference',\n",
       " '_is_label_reference',\n",
       " '_is_level_reference',\n",
       " '_is_mixed_type',\n",
       " '_is_view',\n",
       " '_is_view_after_cow_rules',\n",
       " '_iset_item',\n",
       " '_iset_item_mgr',\n",
       " '_iset_not_inplace',\n",
       " '_item_cache',\n",
       " '_iter_column_arrays',\n",
       " '_ixs',\n",
       " '_logical_func',\n",
       " '_logical_method',\n",
       " '_maybe_align_series_as_frame',\n",
       " '_maybe_cache_changed',\n",
       " '_maybe_update_cacher',\n",
       " '_metadata',\n",
       " '_mgr',\n",
       " '_min_count_stat_function',\n",
       " '_needs_reindex_multi',\n",
       " '_pad_or_backfill',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reduce_axis1',\n",
       " '_reindex_axes',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_rename',\n",
       " '_replace_columnwise',\n",
       " '_repr_data_resource_',\n",
       " '_repr_fits_horizontal_',\n",
       " '_repr_fits_vertical_',\n",
       " '_repr_html_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_sanitize_column',\n",
       " '_series',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_axis_nocheck',\n",
       " '_set_is_copy',\n",
       " '_set_item',\n",
       " '_set_item_frame_value',\n",
       " '_set_item_mgr',\n",
       " '_set_value',\n",
       " '_setitem_array',\n",
       " '_setitem_frame',\n",
       " '_setitem_slice',\n",
       " '_shift_with_freq',\n",
       " '_should_reindex_frame_op',\n",
       " '_slice',\n",
       " '_sliced_from_mgr',\n",
       " '_stat_function',\n",
       " '_stat_function_ddof',\n",
       " '_take_with_is_copy',\n",
       " '_to_dict_of_blocks',\n",
       " '_to_latex_via_styler',\n",
       " '_typ',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'animal',\n",
       " 'any',\n",
       " 'apply',\n",
       " 'applymap',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'assign',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'attrs',\n",
       " 'axes',\n",
       " 'backfill',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'boxplot',\n",
       " 'clip',\n",
       " 'columns',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compare',\n",
       " 'convert_dtypes',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'corrwith',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'droplevel',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'eval',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'explode',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'flags',\n",
       " 'floordiv',\n",
       " 'from_dict',\n",
       " 'from_records',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'info',\n",
       " 'insert',\n",
       " 'interpolate',\n",
       " 'isetitem',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'items',\n",
       " 'iterrows',\n",
       " 'itertuples',\n",
       " 'join',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lt',\n",
       " 'map',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'melt',\n",
       " 'memory_usage',\n",
       " 'merge',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pad',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'pivot',\n",
       " 'pivot_table',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'query',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'rdiv',\n",
       " 'reindex',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'select_dtypes',\n",
       " 'sem',\n",
       " 'set_axis',\n",
       " 'set_flags',\n",
       " 'set_index',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'std',\n",
       " 'style',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_feather',\n",
       " 'to_gbq',\n",
       " 'to_hdf',\n",
       " 'to_html',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_markdown',\n",
       " 'to_numpy',\n",
       " 'to_orc',\n",
       " 'to_parquet',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_records',\n",
       " 'to_sql',\n",
       " 'to_stata',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'to_xml',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'uniq_id',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'value_counts',\n",
       " 'values',\n",
       " 'var',\n",
       " 'water_need',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the class function declarations you can start asking questions to your data.\n",
    "\n",
    "_e.g_ What is the smallest amount of `water_need` for the zoo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.water_need.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, one can find the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoo.water_need.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And eventually, let’s calculate statistical averages, like mean and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 347.72727272727275 and median 325.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean {zoo.water_need.mean()} and median {zoo.water_need.median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "Sometimes you will need to do segmentations on your data.\n",
    "Think about of an experiment where you want to compare different output metrics based on different experimental conditions.\n",
    " \n",
    "For instance, it is nice to know the mean `water_need` of all animals (we have just learned that it is $347.72$).\n",
    "But very often it is much more actionable to break this number down – let’s say – by animal types. \n",
    "With that, we can compare the species to each other – or we can find outliers.\n",
    "\n",
    "Here is a simplified visual that shows how pandas performs “_segmentation_” (grouping and aggregation) based on the column values.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"735\" src=\"https://user-images.githubusercontent.com/49638680/160275058-02b3f727-2143-4205-942a-014e59e2c678.png\">\n",
    "</p>\n",
    "\n",
    "Let’s do the above presented grouping and aggregation for real, on our zoo DataFrame. \n",
    "\n",
    "Speaking of code, we only have to fit in a `groupby` keyword between our zoo variable and our `mean()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>water_need</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elephant</th>\n",
       "      <td>1002.0</td>\n",
       "      <td>550.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kangaroo</th>\n",
       "      <td>1021.0</td>\n",
       "      <td>416.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>1017.5</td>\n",
       "      <td>477.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>1006.0</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra</th>\n",
       "      <td>1012.0</td>\n",
       "      <td>184.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uniq_id  water_need\n",
       "animal                       \n",
       "elephant   1002.0  550.000000\n",
       "kangaroo   1021.0  416.666667\n",
       "lion       1017.5  477.500000\n",
       "tiger      1006.0  310.000000\n",
       "zebra      1012.0  184.285714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoo.groupby('animal').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very important operation. We aggregated by animal unique values (used as indices in the resulting dataframe) and calculated the mean. Here the mean has the role of _aggregation function_. One can use other aggregation functions to get different results.\n",
    "\n",
    "_e.g._ median, count, list, etc.\n",
    "\n",
    "__Small Exercise__: try to implement an aggregation function on your own, taking such that the resulting dataframe will have the 3rd entry of each animal. _Hint_: you can use the `lambda` notation to write functions on a line. \n",
    "\n",
    "Note how `groupby` on its own does not aggregate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.groupby(\"animal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of the returned object is a weird `DataFrameGroupBy`.\n",
    "\n",
    "It has many interesting properties thou. For example, one can iterate over such object and get a dataframe for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in zoo.groupby(\"animal\"):\n",
    "    print(f\"This is the key, e.g. the selected animal: {key}\")\n",
    "    print(f\"This is the df, e.g. the grouped df by animal: \\n {df}\")\n",
    "    print(\"=\"*20)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "1. What is the most frequent source in the travel blog dataframe?\n",
    "2. For the users in `country_2`, what was the most frequent topic and source combination? \n",
    "    Or in other words: which topic, from which source, brought the most views from `country_2`?\n",
    "\n",
    "### Merge\n",
    "In real life data projects, we usually do not store all the data in one big data table. \n",
    "We store it in a few smaller ones instead. \n",
    "There are many reasons behind this; for instance, by using multiple data tables, it is easier to manage your data, easier to avoid redundancy, you can save some disk space, you can query the smaller tables faster, etc.\n",
    "\n",
    "The point is that it is quite usual that during your analysis you have to pull your data from two or more different tables. The solution for that is called _merge_ (a.k.a. “joining” dataframes for sql-ist).\n",
    "\n",
    "Let’s take our zoo dataframe in which we have all our animals, and let’s say that we have another dataframe, `zoo_eats`, that contains information about the food requirements for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe and print it to get a first glance\n",
    "zoo_eats = pd.read_csv(\"datasets/zoo_eats.csv\")\n",
    "zoo_eats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have the animal column, containing the name of the species and the kind of food they eat. Useful information for your zoo manager!\n",
    "\n",
    "We want to merge these two tables into one in order to get all the information in the same place.\n",
    "\n",
    "We can use several way to do that (the most basic one, _strongly_ discouraged is a `for` loop over animal column of `zoo` dataframe). \n",
    "One of the most efficient ones is to use the `merge` method of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, on=\"animal\") # Here the `on` parameter is not strictly necessary as it is the only column the two df's have in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those familiar with SQL, this is equivalent to an _inner join_ where left table is `zoo` and right `zoo_eats`. We could have done the opposite just by exchanging the two dataframes.\n",
    "\n",
    "```python\n",
    "zoo_eats.merge(zoo, on=\"animal\")\n",
    "```\n",
    "\n",
    "This would have changed the order of columns.\n",
    "\n",
    "**Obervation**: Can you see there is no lion 🦁, nor giraffe 🦒? Can you tell why?\n",
    "\n",
    "We can do something more complicated, but first let's revise the kinds of joins we can operate.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"1234\" src=\"https://i2.wp.com/radacad.com/wp-content/uploads/2015/07/joins.jpg\">\n",
    "</p>\n",
    "\n",
    "When you do an `INNER JOIN` (that is the default in pandas), you merge only those values that are found in both tables. \n",
    "On the other hand, when you do the `OUTER JOIN`, it merges all values, even if you can find some of them in only one of the tables.\n",
    "\n",
    "Let’s see a concrete example: did you realize that there is no lion value in zoo_eats? Or that we don’t have any giraffes in zoo? When we did the merge above, by default, it was an INNER merge, so it filtered out giraffes and lions from the result table. But there are cases in which we do want to see these values in our joined dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lions came back 🦁, the giraffe came back 🦒. The only thing is that we have empty (`NaN`) values in those columns where we did not get information from the other table.\n",
    "\n",
    "Let's do some further observation. In this specific case, it might make more sense to keep lions in the table but not the giraffes. Since there are no giraffe in our zoo. In addition, with this choice, we could see all the animals in our zoo and we would have three food categories: vegetables, meat and NaN (which is basically “no information”). \n",
    "\n",
    "In order to do so, we would need to say to the merge method we only want to show animals from `zoo` dataframes, not the `zoo_eats` ones. That is precisely what merging with a `how = 'left'` parameter does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how=\"left\", on=\"animal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more nasty giraffe 🦒!\n",
    "\n",
    "The `how = 'left'` parameter brought all the values from the left table (`zoo`) but brought only those values from the right table (`zoo_eats`) that we have in the left one, too.\n",
    "\n",
    "For doing the merge, pandas needs the key-columns you want to base the merge on (in our case it was the `animal` column in both tables). If you are not so lucky that pandas automatically recognizes these key-columns, or if they have different names you have to help it by providing the column names. That is what the `on`, `left_on` and `right_on` parameters are for.\n",
    "\n",
    "--- \n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Import the tree csv's [customers](https://raw.githubusercontent.com/oscar-defelice/DSAcademy-lectures/master/Lectures_src/01.Pandas/datasets/customers.csv), [products](https://raw.githubusercontent.com/oscar-defelice/DSAcademy-lectures/master/Lectures_src/01.Pandas/datasets/products.csv) and [sales](https://raw.githubusercontent.com/oscar-defelice/DSAcademy-lectures/master/Lectures_src/01.Pandas/datasets/sales.csv).\n",
    "\n",
    "From the above data you are expected to produce following reports.\n",
    "1. List of products sold\n",
    "2. List of quantity sold against each product.\n",
    "3. List of quantity and total sales against each product\n",
    "4. List of quantity sold against each product and against each store.\n",
    "5. List of quantity sold against each Store with total turnover of the store.\n",
    "6. List of products which are not sold\n",
    "7. List of customers who have not purchased any product.\n",
    "\n",
    "_Hint_ You should use Pandas DataFrame methods `merge` and `groupby` to generate these reports.\n",
    "\n",
    "---\n",
    "\n",
    "### Sorting\n",
    "Sorting is essential. The basic sorting method is not too difficult in pandas. \n",
    "The function is called `sort_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values('water_need')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the opposite sorting order it is sufficient to set the boolean parameter `ascending` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values('water_need', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often, you have to sort by multiple columns, so in general, it is recommended using the by keyword for the columns.\n",
    "The list of keys order sets the order of priority in the sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values(by=['animal', 'water_need'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "You may have noticed pandas dataframes have an index structure. This can be retrieved by the attribute `index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the python indexing notation on the `df.loc` and `df.iloc` objects to retrieve entries. \n",
    "Note, `loc` stands for location, while `iloc` stands for _index location_.\n",
    "\n",
    "The main distinction between `loc` and `iloc` is:\n",
    "\n",
    "* `loc` is label-based, which means that you have to specify rows and columns based on their row and column _labels_.\n",
    "* `iloc` is integer position-based, so you have to specify rows and columns by their _integer position values_ (0-based integer position).\n",
    "\n",
    "We report here a table to collect differences and similarities.\n",
    "<p align=\"center\">\n",
    "    <img width=\"1000\" src=\"https://miro.medium.com/max/1400/1*CgAWzayEQY8PQuMpRkSGfQ.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting via a single value\n",
    "Both loc and iloc allow input to be a single value. We can use the following syntax for data selection:\n",
    "* `loc[row_label, column_label]`\n",
    "* `iloc[row_position, column_position]`\n",
    "\n",
    "When index is numeric, like in our `zoo` example `loc` and `iloc` on rows behaves in the same way.\n",
    "Let's consider the groupby result thou.\n",
    "\n",
    "For example, let’s say we would like to retrieve the tiger water need mean value.\n",
    "With loc, we can pass the row label 'tiger' and the column label 'water_need'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zoo = zoo.groupby(\"animal\").mean()\n",
    "grouped_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zoo.loc[\"tiger\", 'water_need']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent `iloc` statement should take the row number `3` and the column number `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zoo.iloc[3,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is plenty of other selection choices, hence we refer to [this nice post](https://towardsdatascience.com/how-to-use-loc-and-iloc-for-selecting-data-in-pandas-bd09cb4c3d79) to summarise a further couple of them.\n",
    "\n",
    "#### Reset_index\n",
    "\n",
    "Now that we are aware of the indexing structure of dataframes, one may feel in need to reset index order, _e.g._ you may have noticed that after a sorting operation it can happen that all the indexes become shuffled.\n",
    "\n",
    "Wrong indexing can mess up your visualizations or even your machine learning models.\n",
    "\n",
    "The point is: in certain cases, when you have done a transformation on your dataframe, you have to re-index the rows. For that, you can use the `reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values(by=['water_need'], ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our new dataframe kept the old indexes, too. \n",
    "If you want to remove them, just add the `drop = True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values(by=['water_need'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fillna\n",
    "\n",
    "Let’s rerun the left-merge method that we have used above. The `NaN` values appearing in lions rows may be disturbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that we have NaN values for lions. `NaN` itself can be really distracting, so one can choose to replace those values with something more meaningful. In some cases, this can be a $0$ value, or in other cases a specific string value. In this case even if the `zoo_eat` dataframe gave us no clue about the lion diet, we remember from kindergarden that lions liek meat.\n",
    "Let’s use the `fillna` method, which basically finds and replaces all `NaN` values in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how='left').fillna('meat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Exercise \n",
    "Download a new dataset and store the data in a dataframe named `blog_buy`, using the instructions below; \n",
    "The previously seen dataset (stored in `pandas_tutorial_read.csv`) shows all the users who read an article on the blog, while the `blog_buy` dataset shows all the users who bought something on the very same blog between `2018-01-01` and `2018-01-07`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget 46.101.230.157/dilan/pandas_tutorial_buy.csv\n",
    "mv pandas_tutorial_buy.csv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_buy = pd.read_csv('datasets/pandas_tutorial_buy.csv', delimiter=';', names=[\n",
    "                       'my_date_time', 'event', 'user_id', 'amount'], \n",
    "                       parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What’s the average (mean) revenue between 2018-01-01 and 2018-01-07 from the users in the `pandas_tutorial_read.csv` dataset?\n",
    "2. Print the top 3 countries by total revenue between 2018-01-01 and 2018-01-07\n",
    "\n",
    "\n",
    "**Answers**\n",
    "1. The average revenue between 2018-01-01 and 2018-01-07 from the users in the `pandas_tutorial_read.csv` dataset is ~$102.1$.\n",
    "2. The top 3 countries by revenue are\n",
    "\n",
    "| Country | Total Revenue |\n",
    "|---|----|\n",
    "| Country 4  |  $1112.0$    | \n",
    "| Country 5  |  $324.0$     | \n",
    "| Country 2  |  $296.0$     | \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## A data analysis\n",
    "\n",
    "Furthermore we are going to use pandas tools to perform an _exploratory data analysis_ over a dataset.\n",
    "\n",
    "### Import data\n",
    "\n",
    "An interesting feature of read methods in pandas are that it is allowed to give them a url and they will read data from it.\n",
    "\n",
    "#### The dataset\n",
    "\n",
    "We are going to use a famous dataset, the notorious [IMBD movies dataset]().\n",
    "The IMDB movie reviews dataset is a set of reviews, there are various versions of it, one can read more about the version used in these lectures in the [official documentation](https://files.grouplens.org/datasets/movielens/ml-latest-README.html). \n",
    "The dataset is available online and can be either directly downloaded from Stanford’s website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/LearnDataSci/articles/master/Python%20Pandas%20Tutorial%20A%20Complete%20Introduction%20for%20Beginners/IMDB-Movie-Data.csv\", index_col=\"Title\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can check the dimensions by a method inherited by numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset does not have duplicate rows, but it is always important to verify you are working with duplicates.\n",
    "\n",
    "To demonstrate, let's simply just double up our movies DataFrame by appending it to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.append(df)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `append` will return a copy without affecting the original DataFrame. We are capturing this copy in temp so we aren't working with the real data.\n",
    "\n",
    "Now we can try dropping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.drop_duplicates()\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like `append`, the `drop_duplicates` method will also return a copy of your `DataFrame`, but this time with duplicates removed.\n",
    "\n",
    "Another important argument for `drop_duplicates` is `keep`, which has three possible options:\n",
    "\n",
    "1. `\"first\"`: (default) Drop duplicates except for the first occurrence.\n",
    "2. `\"last\"`: Drop duplicates except for the last occurrence.\n",
    "3. `False`: Drop all duplicates.\n",
    "\n",
    "Since we did not indicate the keep argument in the previous example it was defaulted to `\"first\"`. This means that if two rows are the same pandas will drop the second row and keep the first one. \n",
    "Using `last` has the opposite effect: the first row is dropped.\n",
    "\n",
    "`keep = False`, on the other hand, will drop all duplicates. If two rows are the same then both will be dropped. \n",
    "Let's see what happens to `temp_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.append(df)  # make a new copy\n",
    "temp_df.drop_duplicates(inplace=True, keep=False)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all rows were duplicates, `keep=False` dropped them all resulting in zero rows being left over.\n",
    "\n",
    "### Column clean up\n",
    "\n",
    "Many times datasets will have verbose column names with symbols, upper and lowercase words, spaces, and typos. To make selecting data by column name easier we can spend a little time cleaning up their names.\n",
    "\n",
    "Here is how to print the column names of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does `columns` come in handy if you want to rename columns by allowing for simple copy and paste, it is also useful if you need to understand why you are receiving a Key Error when selecting data by column.\n",
    "\n",
    "We can use the `rename` method to rename certain or all columns via a dict. We do not want parentheses, so let's rename those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'Runtime (Minutes)': 'Runtime',\n",
    "    'Revenue (Millions)': 'Revenue_millions'\n",
    "})\n",
    "\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. But what if we want to lowercase all names? \n",
    "Instead of using `rename` we could also reassing the column attribute to a list of names like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.lower() for col in df]\n",
    "\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "\n",
    "When exploring data, one most likely encounters missing or null values, which are essentially placeholders for non-existent values. \n",
    "Most commonly in these contexts one faces Python's `None` or NumPy's `np.nan`, each of which have to be handled differently according to situations.\n",
    "\n",
    "There are two options in dealing with nulls:\n",
    "\n",
    "1. Get rid of rows or columns with nulls\n",
    "2. Replace nulls with non-null values, a technique known as imputation\n",
    "\n",
    "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our DataFrame are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice `isnull` returns a DataFrame where each cell is either `True` or `False` depending on that cell's null status.\n",
    "\n",
    "__Quick Exercise__: Starting from the boolean dataframe above, count the number of nulls in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_series = # YOUR CODE HERE\n",
    "\n",
    "print(f\"The result series with a count of null values per column. \\n {result_series}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have found now that your data has $128$ missing values for `revenue_millions` and $64$ missing values for `metascore`.\n",
    "\n",
    "Data Scientists and Analysts regularly face the dilemma of dropping or imputing null values, and is a decision that requires intimate knowledge of your data and its context. \n",
    "Overall, removing null data is only suggested if you have a small amount of missing data.\n",
    "\n",
    "Beside moral issues, removing nulls is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna() # This drops the whole line where a NaN appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will delete any row with at least a single null value, but it will return a new DataFrame without altering the original one.\n",
    "\n",
    "So in the case of our dataset, this operation would remove $128$ rows where `revenue_millions` is null and $64$ rows where `metascore` is null (there might be an intersection).\n",
    "\n",
    "Other than just dropping rows, you can also drop columns with null values by setting `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, this operation would drop the `revenue_millions` and `metascore` columns.\n",
    "\n",
    "##### Imputing\n",
    "Imputation is a conventional feature engineering technique used to keep valuable data that have null values.\n",
    "\n",
    "There may be instances where dropping every row with a null value removes too big a chunk from your dataset, so instead we can impute that null with another value, usually the mean or the median of that column.\n",
    "\n",
    "Let's look at imputing the missing values in the `revenue_millions` column. First we will extract that column series into its own variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenues = df['revenue_millions']\n",
    "revenues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly different formatting than a DataFrame, but we still have our Title index.\n",
    "\n",
    "We will impute the missing values of revenue using the mean.\n",
    "\n",
    "__Quick Exercise__: Find the mean of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenues_mean = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to replace NaN values with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenues = revenues.fillna(revenues_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Imputing an entire column with the same value like this is a basic example. \n",
    "It would be a better idea to try a more granular imputation by Genre or Director.\n",
    "\n",
    "1. Find the mean of the revenues generated in each genre individually and impute the nulls in each genre with that genre's mean.\n",
    "2. Find the mean of the revenues generated in each director individually and impute the nulls of each movie with that director's mean.\n",
    "\n",
    "---\n",
    "\n",
    "### Applying functions\n",
    "\n",
    "This is one of the most useful properties in these lectures. Indeed, the needing of operating in a involuted way on dataframes entries is ubiquitous. \n",
    "The iteration over a DataFrame or Series as you would with a list is possible, however, because of the complex structure of a dataframe, this is really not efficient.\n",
    "\n",
    "An efficient alternative is to `apply` a function to the dataset. For example, we could use a function to convert movies with an 8.0 or greater to a string value of `\"good\"` and the rest to `\"bad\"` and use this transformed values to create a new column.\n",
    "\n",
    "First we would create a function that, when given a rating, determines if it's good or bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_function(x, value = 8.0):\n",
    "    if x >= value:\n",
    "        return \"good\"\n",
    "    else:\n",
    "        return \"bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to send the entire rating column through this function, which is what `apply` does. This operation is _vectorised_ so taking advantage of the pandas structures, it is more efficient than an explicit for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating_category\"] = df[\"rating\"].apply(rating_function, value = 8.0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` method passes every value in the rating column through the `rating_function` and then returns a new Series. This Series is then assigned to a new column called `rating_category`.\n",
    "\n",
    "You can also use anonymous functions as well. This `lambda` function achieves the same result as `rating_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating_category\"] = df[\"rating\"].apply(\n",
    "    lambda x: 'good' if x >= 8.0 else 'bad')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas pipe\n",
    "\n",
    "There is a great method to apply multiple transformation in an efficient and compact way: [`pipe`](https://pandas.pydata.org/pandas-docs/version/1.0.0/reference/api/pandas.DataFrame.pipe.html).\n",
    "\n",
    "The best way to illustrate it is through an example.\n",
    "\n",
    "First let's define a bunch of function to apply. The important property is they must all have as parameter and return the dataframe.\n",
    "You can see the pipe as a transformation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def remove_null(df):\n",
    "    \"\"\"remove null values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        the base dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        the dataframe with null values removed\n",
    "    \"\"\"\n",
    "    return df.dropna()\n",
    "\n",
    "def drop_genre(df, genre):\n",
    "    \"\"\"remove a specific genre movies\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        the base dataframe\n",
    "    \n",
    "    genre: str\n",
    "        the movie genre to be removed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        the filtered dataframe \n",
    "    \"\"\"\n",
    "    series_remove = df.genre.str.contains(f\"{genre}\") # Boolean series\n",
    "    idx_to_drop = series_remove[series_remove].index # Masked series indices\n",
    "    return df.drop(index=idx_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily build a pipeline over a dataframe by calling `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.pipe(remove_null)\n",
    "    .pipe(drop_genre, \"Drama\")) # We do not like dramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Using apply method to add a further column to the dataframe. The added column contains boolean values indicating whether the director is also an actor in the movie. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "258d6f942e9abff99338ee3ea05bb7abc0fd3eb4d49f988f84979247168b5568"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('lectures')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
