{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICN Programming Course\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"500\" alt=\"image\" src=\"https://github.com/Lenakeiz/ICN_Programming_Course/blob/main/Images/cog_neuro_logo_blue_png_0.png?raw=true\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## Pandas\n",
    "In this part of the lesson we are going to present one of the tools you will most likely use for .\n",
    "\n",
    "[Pandas library](https://pandas.pydata.org/) defines and makes use of a new _data structure_, _i.e._ the `DataFrame`. \n",
    "Actually pandas define more than just a data structure, for instance we will make use of `Series` and examine the difference with dataframes.\n",
    "\n",
    "### Advantages of pandas\n",
    "\n",
    "Data scientists use Pandas for its following advantages:\n",
    "\n",
    "* Easily handles missing data;\n",
    "* It provides an efficient way to slice the data;\n",
    "* It provides a flexible way to merge, concatenate or reshape the data;\n",
    "* It includes a powerful data casting tool to work with;\n",
    "* It wraps data visualisation libraries in order to quickly plotting analysis results.\n",
    "\n",
    "Tthe main disadvantage of pandas is that it is relegated to manipulate dataframes whose dimension is strictly lower than memory.\n",
    "### Dataframes\n",
    "\n",
    "A `DataFrame` is a table. As any other type in python, it is defined as a class, with its attributes and methods. \n",
    "You can check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to see the class APIs and see it s capability\n",
    "\n",
    "More formally, it is a rank-$2$ array, with axes labelled as _rows_ and _columns_.\n",
    "It is the basic object in pandas and a really common way to load data in memory in order to operate on them.\n",
    "\n",
    "Now the question you are all wondering: how to _create a dataframe_. There are several ways, by tuples, by lists, by numpy arrays or even by dictionaries. \n",
    "As a first instance, let's consider a list of names corresponding to people and their age, you can create a data frame in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pd is a standard alias for pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of lists made by [str, int]\n",
    "lst = [['mario', 25], ['billy', 30], ['lakitu', 26], ['bowser', 22]]\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['Name', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict made by {str: int}\n",
    "data_dict = {'mario': 25, 'billy': 30, 'lakitu': 26, 'bowser': 22}\n",
    "\n",
    "df = pd.DataFrame(data=data_dict.items(), columns=['Name', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from csv file\n",
    "df = pd.read_csv('datasets/people.csv', header=None, names=['Name', 'Age'])\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from a numpy array\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([['mario', 25], ['billy', 30], ['lakitu', 26], ['bowser', 22]])\n",
    "df = pd.DataFrame(arr, columns=['Name', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have the exact same object.\n",
    "Once data are organised in the dataframe, no matter how we imported them, they are stored in that object that has always the same methods and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "A `Series` is a one-dimensional data structure. It can have any data structure like integer, float, and string, or even composite ones like lists, dictionaries, etc. \n",
    "\n",
    "It is useful when you want to perform computation or return a one-dimensional array.\n",
    "A series, by definition, cannot have multiple columns.\n",
    "For the latter case, use the data frame structure, which indeed can be considered as made up by series.\n",
    "\n",
    "Series has one parameters, the data, that can be a list, a dictionary, or a scalar value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1., 2., 3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from files\n",
    "\n",
    "Data can be loaded in a DataFrame from different data format, like csv, xlx, json, etc.\n",
    "\n",
    "The most common file type is the csv (comma separated values).\n",
    "You can load them into a `Dataframe` using `read_csv` method, eventually specifying also the type of delimeter used for separating the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel = pd.read_csv('datasets/travel_blog_data.csv', delimiter=';')\n",
    "df_travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset holds the user activity data from a travel blog. \n",
    "\n",
    "It is noteworthy a default behaviour in pandas `read_csv`.\n",
    "The csv file do not have a header row, therefore pandas used the first row of data as header; in order to set the name of the columns you can use the `name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel = pd.read_csv('datasets/travel_blog_data.csv', delimiter=';',\n",
    "                 names=['timestamp', 'event', 'country', 'user_id', 'source', 'topic'],\n",
    "                 parse_dates=True)\n",
    "df_travel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it might be handy not to print the whole dataframe and flood your screen with data. \n",
    "When a few lines is enough, you can print only the first $n$ lines ‚Äì by typing:\n",
    "\n",
    "```python\n",
    "df_travel.head(n)\n",
    "```\n",
    "\n",
    "If you leave the $n$ parameter blank, the method takes the default value, that is $5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By symmetry, you can imagine what the `tail` method returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also need a random sampling of $k$ lines out of the dataframe, this can be achieved by the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other two dataframe methods that are very useful in analysing data are `describe` and `info`.\n",
    "\n",
    "The `describe` method allows to get some statistical information about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_travel.describe()))\n",
    "df_travel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the result is again a dataframe whose index is a list of statistical properties, and as columns the values of indexed properties for the first available numerical column (in our case `user_id`).\n",
    "\n",
    "As one can read in the [docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) the method returns a Summary statistics of the Series or Dataframe provided.\n",
    "\n",
    "Again, from the official documentation \n",
    "\n",
    "> For numeric data, the result‚Äôs index will include `count`, `mean`, `std`, `min`, `max` as well as lower, $50$ and upper percentiles. By default the lower percentile is $25$ and the upper percentile is $75$. The $50$ percentile is the same as the median.\n",
    ">\n",
    "> For object data (e.g. strings or timestamps), the result‚Äôs index will include `count`, `unique`, `top`, and `freq`. The `top` is the most common value. The `freq` is the most common value‚Äôs frequency. `Timestamps` also include the first and last items.\n",
    ">\n",
    "> If multiple object values have the highest count, then the `count` and `top` results will be arbitrarily chosen from among those with the highest count.\n",
    ">\n",
    "> _For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type._\n",
    ">\n",
    "> The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.\n",
    "\n",
    "We can however try to override the default behaaviour and we can do so by setting `include='all'` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, we also have `info` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_info  = df_travel.info()\n",
    "dataframe_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method actually returns a `NoneType`. But in the execution _prints_ on screen some information about the dataframe. \n",
    "It is less informative (almost not informative at all) from the statistics point of view, but it tells us some numerical property of the dataframe, indeed this method prints information about the DataFrame including the index `dtype` and columns, non-null values and memory usage.\n",
    "\n",
    "Interesting enough the first colum however should be able to be parsed into an object more easy to read. \n",
    "In particular, since the column clearly represents a date we can transform it into an object that is more recognisibile.\n",
    "We can parse it into `datetime` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dadtaframe function to_datetime() to convert the timestamp column to datetime\n",
    "# Saving the output directly to the timestamp column by replacing it\n",
    "df_travel['timestamp'] = pd.to_datetime(df_travel['timestamp'])\n",
    "df_travel.info()\n",
    "\n",
    "# Extracting date and time from the first row\n",
    "first_row_timestamp = df_travel.loc[0, 'timestamp']\n",
    "extracted_date = first_row_timestamp.date()\n",
    "extracted_time = first_row_timestamp.time()\n",
    "\n",
    "print(\"Date from the first row:\", extracted_date)\n",
    "print(\"Time from the first row:\", extracted_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last descriptive method we present here is the `corr` one. \n",
    "By using such a method we can generate the relationship between each numerical variables.\n",
    "This function will throw an error if you load a dataset that does not contain numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = pd.read_csv('datasets/memory_recall_test.csv', delimiter=',')\n",
    "df_memory.head()\n",
    "\n",
    "correlation_matrix = df_memory.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by selecting columns\n",
    "\n",
    "Sometimes you will need to only work with specific columns from a dataset.\n",
    "You can do so by using the two following and equivalent syntaxes.\n",
    "The second syntax is less flexible as it required the column names to be in a certain way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way, the square bracket notation\n",
    "age = df_memory['Age (Years)']\n",
    "print(type(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second way, the point notation\n",
    "# the column name must not contain spaces, special characters, or starts with a number\n",
    "# if you want to use dot notation, you would need to rename the column to remove spaces and special characters.\n",
    "# for example:\n",
    "df_memory.rename(columns={'Age (Years)': 'Age_years'}, inplace=True)\n",
    "\n",
    "# the inplace parameter is used to modify the dataframe in place, without creating a new dataframe object otherwise you would need to assign the output to a new variable\n",
    "# new_df_memory = df_memory.rename(columns={'Age (Years)': 'Age_years'})\n",
    "age = df_memory.Age_years\n",
    "print(type(age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "‚ö†Ô∏è Both of the previous syntaxes return a <em>Series</em> rather than a <em>Dataframe</em>\n",
    "</div>\n",
    "\n",
    "If you want a dataframe, you need to slightly change the previous commands in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard way, no one uses that.\n",
    "pd.DataFrame(df_memory.Age_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy way, it generalises easily to the multi-column case.\n",
    "age = df_memory[['Age_years']]\n",
    "print(type(age))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about multi-column filter?\n",
    "\n",
    "As the previous cell might suggest, you only need to pass a list of columns.\n",
    "\n",
    "Note the double bracket `[[]]`, you can consider `[]` as a _filter_ operator, whose argument is the `list` of column names.\n",
    "Recall that a `Series` admits only one column, hence the result of this operation cannot be other than a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just reloading the data to make sure we have the original data\n",
    "df_travel = pd.read_csv('datasets/travel_blog_data.csv', delimiter=';',\n",
    "                 names=['timestamp', 'event', 'country', 'user_id', 'source', 'topic'],\n",
    "                 parse_dates=True)\n",
    "\n",
    "df_travel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_travel[['user_id','country']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of names changes the order in the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel[['country', 'user_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rows on values\n",
    "\n",
    "There is complementary way of filtering a dataframe, on rows value. Hence, we can reduce the number of records in the dataframe based on some condition.\n",
    "\n",
    "Let's use the imnported travel dataframe, and for instance, you want to see the entries corresponding to the users who came from the \"SEO\" source. In this case you have to filter for the \"SEO\" value in the \"source\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel[df_travel.source == 'SEO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand the command above, let's focus on how pandas interpret the filtering procedure.\n",
    "\n",
    "**Step 1**: First, between the bracket frames `[]` it evaluates every line: is the `df.source` column‚Äôs value `'SEO'` or not? The results are boolean values (True or False), better a `Series` of boolean values. \n",
    "Indeed, we have seen how `df.source` is a series, a comparison with a value (through the binary operator `==`) will produce a truth-value object of the same type of `df.source` hence a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note the dtype attribute\n",
    "df_travel.source == 'SEO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 2**: The previous boolean series is what is called a _mask_.\n",
    "If we filter through a mask, the filtered dataframes returns every row where the mask is `True` and drops any row where it is `False`.\n",
    "This is equivalent to the logical indexing in `Matlab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A less concise, but maybe clearer notation\n",
    "mask_seo = (df_travel.source == 'SEO') # Boolean series\n",
    "print(type(mask_seo))\n",
    "df_travel[mask_seo] # Masks away the rows corresponding to \"False\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious now that you can combine more conditions to end up into a boolean mask and apply even complicated filter.\n",
    "\n",
    "_Example_: We want to filter the dataframe to get all the users coming from a \"SEO\" source, with topic related to \"Asia\" and with a timestamp between 23.00 and 23.30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_mask = ((df_travel.source == 'SEO') & (df_travel.topic == 'Asia') & (df_travel.timestamp >= '2018-01-01 23:00:00') & (df_travel.timestamp <= '2018-01-01 23:30:00'))\n",
    "df_travel[bool_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new columns\n",
    "\n",
    "Creating a new column is easy, you simply _declare_ the new column name as `df[\"new_col\"] =  new_col` where `new_col` is a pandas Series.\n",
    "The new column will be filled with `NaN`` (Not a Number, which is Pandas' standard missing data marker) for all rows where the Series does not have a corresponding index.\n",
    "\n",
    "You can also calculate the new column entries by operating on existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column as the string concatenation of \"country\" and \"topic\"\n",
    "df_travel[\"contry_code_per_continent\"] = df_travel.country + \"||\" + df_travel.topic\n",
    "df_travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a further column made by the 2nd letter of the string contained in the \"event\" column\n",
    "df_travel[\"part_string\"] = df_travel.event.str[1]\n",
    "df_travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential filtering and masks \n",
    "\n",
    "Filter and mask operations can be used sequentially, one after the other.\n",
    "\n",
    "It is very important to understand that pandas‚Äôs logic is linear. \n",
    "So if you apply a function, you can always apply another one on the result. \n",
    "In this case, the input of the latter function will always be the output of the previous one. \n",
    "This will have a very nice consequence in a construct called `pipe` (see later for details).\n",
    "\n",
    "As you have already seen, for instance, we can combine two selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executed head() first then apply the column selection\n",
    "df_travel.head()[['country', 'user_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line first selects the first $5$ rows of our dataframe. And then it takes only the ‚Äòcountry‚Äô and the ‚Äòuser_id‚Äô columns.\n",
    "\n",
    "As these operations are commutative, you would not be surprised by the fact that one can get the same result with the reversed chain of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel[['country', 'user_id']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, you select the columns first, then take the first five rows. \n",
    "The result is the same ‚Äì just the order of the functions (and the execution) is different.\n",
    "\n",
    "_Try to think which one is better in terms of computational time._\n",
    "\n",
    "We can check it by running some very simple test using the Python in-built package time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "# This dataset is quite large so it s not part of the files you can download from the course page.\n",
    "\n",
    "df_large_dataset = pd.read_csv('./datasets/large_dataset.csv', delimiter=',')\n",
    "print(df_large_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize total time counters\n",
    "total_time_first_method = 0\n",
    "total_time_second_method = 0\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "for _ in range (iterations):\n",
    "    # Measure time for df_travel[['country', 'user_id']].head()\n",
    "    start_time = time.time()\n",
    "    df_large_dataset[['country', 'user_id']].head(100)\n",
    "    end_time = time.time()\n",
    "    total_time_first_method += end_time - start_time\n",
    "\n",
    "    # Measure time for df_travel.head()[['country', 'user_id']]\n",
    "    start_time = time.time()\n",
    "    df_large_dataset.head(100)[['country', 'user_id']]\n",
    "    end_time = time.time()\n",
    "    total_time_second_method += end_time - start_time\n",
    "\n",
    "# Compute average time for each method\n",
    "time_first_method = total_time_first_method / iterations\n",
    "time_second_method = total_time_second_method / iterations\n",
    "\n",
    "print(f\"Time for first method: {time_first_method:6f} seconds\")\n",
    "print(f\"Time for second method: {time_second_method:6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a lot of interesting ways of selecting columns out of a dataframe. \n",
    "Look at this [nice post](https://towardsdatascience.com/interesting-ways-to-select-pandas-dataframe-columns-b29b82bbfb33#:~:text=Selecting%20columns%20based%20on%20their,Returns%20a%20pandas%20series.&text=Passing%20a%20list%20in%20the,columns%20at%20the%20same%20time.) in order to look at some non-standard examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "In Python, particularly in the context of data analysis with libraries like Pandas, \"aggregation\" refers to the process of combining multiple data points into a single value based on a specified criterion or operation.\n",
    "This process is crucial in data analysis for summarizing data, extracting insights, and simplifying complex datasets.\n",
    "\n",
    "Let's consider a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo = pd.read_csv(\"datasets/zoo.csv\")\n",
    "zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of the animals is as easy as applying a `count` function on the zoo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can observe, the `count()` method counts the number of values in each column. \n",
    "In the case of the zoo dataset, there were $3$ columns, and each of them had $22$ values in it.\n",
    "\n",
    "If you want to make your output clearer, you can select the animal column first by using one of the selection operators from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo[['animal']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in this particular case, the result could be even nicer for Series, in fact to show just the number and not the column name nor the type, one can use the following syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.animal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as before, one can notice how pandas applied functions sequentially. \n",
    "First, `zoo.animal` is a Series, then calculated the count of it. \n",
    "\n",
    "Indeed an equivalent, even if less compact notation, for this is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_series = zoo.animal\n",
    "animal_series.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same logic, one can sum all the values inside a column by the `sum` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.water_need.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there are a lot of methods and one should know in advance what they are and how they works.\n",
    "\n",
    "In order to get such information, the royal road is to read the documentation. However, a convenient way to get a list of all available methods of a class instance is the function `dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the class function declarations you can start asking questions to your data.\n",
    "\n",
    "_e.g_ What is the smallest amount of `water_need` for the zoo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.water_need.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, one can find the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.water_need.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And eventually, let‚Äôs calculate statistical averages, like mean and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean {zoo.water_need.mean()} and median {zoo.water_need.median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "Sometimes you will need to do segmentations on your data.\n",
    "Think about of an experiment where you want to compare different output metrics based on different experimental conditions.\n",
    " \n",
    "For instance, it is nice to know the mean `water_need` of all animals (we have just learned that it is $347.72$).\n",
    "But very often it is much more actionable to break this number down ‚Äì let‚Äôs say ‚Äì by animal types. \n",
    "With that, we can compare the species to each other ‚Äì or we can find outliers.\n",
    "\n",
    "Here is a simplified visual that shows how pandas performs ‚Äú_segmentation_‚Äù (grouping and aggregation) based on the column values.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"735\" src=\"https://github.com/Lenakeiz/ICN_Programming_Course/blob/main/week_7/images/grouping_concepts.png?raw=true\">\n",
    "</p>\n",
    "\n",
    "Let‚Äôs apply grouping on our zoo DataFrame. \n",
    "\n",
    "Speaking of code, we only have to fit in a `groupby` keyword between our zoo variable and our `mean()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.groupby('animal').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very important operation. We aggregated by animal unique values (used as indices in the resulting dataframe) and calculated the mean.\n",
    "Here the mean has the role of _aggregation function_.\n",
    "One can use other aggregation functions to get different results.\n",
    "\n",
    "_e.g._ median, count, list, etc.\n",
    "\n",
    "For example we could try to implement a custom metric that we want to apply after the aggregation function that takes only the 3rd entry from each given animal. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "‚ÑπÔ∏è you can use the <em>lambda</em> notation to write functions on a line.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lambda function to extract the third entry of each animal\n",
    "# We'll group by 'animal' and then apply the function to get the third entry\n",
    "third_entry = lambda x: x.iloc[2] if len(x) > 2 else None\n",
    "\n",
    "# Apply the aggregation function to the grouped DataFrame\n",
    "third_entry_df = zoo.groupby('animal').apply(third_entry, include_groups=False)\n",
    "third_entry_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note how `groupby` on its own does not aggregate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zoo_group_by = zoo.groupby(\"animal\")\n",
    "print(type(df_zoo_group_by))\n",
    "df_zoo_group_by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of the returned object is a `DataFrameGroupBy`.\n",
    "It does not have practical uses, apart from the fact that can be iterated to get a dataframe for each group.\n",
    "\n",
    "This can be very useful if you are planning to do additional analysis on each of the grouped dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in zoo.groupby(\"animal\"):\n",
    "    print(f\"This is the key, e.g. the selected animal: {key}\")\n",
    "    print(f\"This is the df, e.g. the grouped df by animal: \\n {df}\")\n",
    "    print(\"=\"*20)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "In our experiments, we usually do not store all the data in one big data table.\n",
    "\n",
    "We store it in a few smaller ones instead, usually each data table consisting on the data for each single participant.\n",
    "\n",
    "Apart from the practicality of doing so, there are many reasons behind this; for instance, by using multiple data tables, it is easier to manage your data, easier to avoid redundancy, you can save some disk space, you can query the smaller tables faster, etc.\n",
    "\n",
    "The point is that it is quite usual that during your analysis you have to pull your data from two or more different tables.\n",
    "The solution for that is called _merge_ (in computer programmming this is very similar to use `structured query language (SQL)` on databases).\n",
    "\n",
    "Let‚Äôs take our zoo dataframe in which we have all our animals, and let‚Äôs say that we have another dataframe, `zoo_eats`, that contains information about the food requirements for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import dataframe and print it to get a first glance\n",
    "zoo_eats = pd.read_csv(\"datasets/zoo_eats.csv\")\n",
    "zoo_eats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have the animal column, containing the name of the species and the kind of food they eat.\n",
    "\n",
    "We want to merge these two tables into one in order to get all the information in the same place.\n",
    "\n",
    "We can use several way to do that (the most basic one, _strongly_ discouraged is a `for` loop over animal column of `zoo` dataframe). \n",
    "One of the most efficient ones is to use the `merge` method of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, on=\"animal\") # Here the `on` parameter is not strictly necessary as it is the only column the two df's have in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those familiar with SQL, this is equivalent to an _inner join_ where left table is `zoo` and right `zoo_eats`.\n",
    "We could have done the opposite just by exchanging the two dataframes.\n",
    "\n",
    "```python\n",
    "zoo_eats.merge(zoo, on=\"animal\")\n",
    "```\n",
    "\n",
    "This would have just changed the order of columns.\n",
    "\n",
    "**Obervation**: Can you see there is no lion ü¶Å, nor giraffe ü¶í? Can you tell why?\n",
    "\n",
    "The followings are the type of joins that you can operate similar to SQL.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"600\" src=\"https://github.com/Lenakeiz/ICN_Programming_Course/blob/main/week_7/images/joins-sql.png?raw=true\">\n",
    "</p>\n",
    "\n",
    "When you do an `INNER JOIN` (that is the default in pandas), you merge only those values that are found in both tables. \n",
    "On the other hand, when you do the `OUTER (FULL) JOIN`, it merges all values, even if you can find some of them in only one of the tables.\n",
    "\n",
    "Let‚Äôs see a concrete example: did you realize that there is no lion value in zoo_eats? Or that we don‚Äôt have any giraffes in zoo? When we did the merge above, by default, it was an INNER merge, so it filtered out giraffes and lions from the result table. But there are cases in which we do want to see these values in our joined dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lions came back ü¶Å, the giraffe came back ü¶í.\n",
    "\n",
    "The only thing is that we have empty (`NaN`) values in those columns where we did not get information from the other table.\n",
    "\n",
    "Let's do some further observation.\n",
    "In this specific case, it might make more sense to keep lions in the table but not the giraffes.\n",
    "Since there are no giraffe in our zoo.\n",
    "In addition, with this choice, we could see all the animals in our zoo and we would have three food categories: vegetables, meat and NaN (which is basically ‚Äúno information‚Äù). \n",
    "\n",
    "In order to do so, we would need to say to the merge method we only want to show animals from `zoo` dataframes, not the `zoo_eats` ones. That is precisely what merging with a `how = 'left'` parameter does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how=\"left\", on=\"animal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more giraffe ü¶í!\n",
    "\n",
    "The `how = 'left'` parameter brought all the values from the left table (`zoo`) but brought only those values from the right table (`zoo_eats`) that we have in the left one, too.\n",
    "\n",
    "For doing the merge, pandas needs the key-columns you want to base the merge on (in our case it was the `animal` column in both tables). If you are not so lucky that pandas automatically recognizes these key-columns, or if they have different names you have to help it by providing the column names. That is what the `on`, `left_on` and `right_on` parameters are for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "Sorting is essential. The basic sorting method is not too difficult in pandas. \n",
    "The function is called `sort_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values('water_need')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the opposite sorting order it is sufficient to set the boolean parameter `ascending` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values('water_need', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often, you have to sort by multiple columns, so in general, it is recommended using the by keyword for the columns.\n",
    "The list of keys order sets the order of priority in the sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values(by=['animal', 'water_need'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "You may have noticed pandas dataframes have an index structure. This can be retrieved by the attribute `index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the python indexing notation on the `df.loc` and `df.iloc` objects to retrieve entries. \n",
    "Note, `loc` stands for location, while `iloc` stands for _index location_.\n",
    "\n",
    "The main distinction between `loc` and `iloc` is:\n",
    "\n",
    "* `loc` is label-based, which means that you have to specify rows and columns based on their row and column _labels_.\n",
    "* `iloc` is integer position-based, so you have to specify rows and columns by their _integer position values_ (0-based integer position).\n",
    "\n",
    "We report here a table to collect differences and similarities.\n",
    "<p align=\"center\">\n",
    "    <img width=\"1000\" src=\"https://miro.medium.com/max/1400/1*CgAWzayEQY8PQuMpRkSGfQ.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.iloc[lambda x: x.index % 2 == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting via a single value\n",
    "Both loc and iloc allow input to be a single value. We can use the following syntax for data selection:\n",
    "* `loc[row_label, column_label]`\n",
    "* `iloc[row_position, column_position]`\n",
    "\n",
    "When index is numeric, like in our `zoo` example `loc` and `iloc` on rows behaves in the same way.\n",
    "Let's consider the groupby results.\n",
    "\n",
    "For example, let‚Äôs say we would like to retrieve the 'tiger' water need mean value.\n",
    "With loc, we can pass the row label 'tiger' and the column label 'water_need'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zoo = zoo.groupby(\"animal\").mean()\n",
    "grouped_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zoo.loc[\"tiger\", 'water_need']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent `iloc` statement should take the row number `3` and the column number `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zoo.iloc[3,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is plenty of other selection choices, hence we refer to [this nice post](https://towardsdatascience.com/how-to-use-loc-and-iloc-for-selecting-data-in-pandas-bd09cb4c3d79) to summarise a further couple of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Reset_index\n",
    "\n",
    "Now that we are aware of the indexing structure of dataframes, one may feel in need to reset index order, _e.g._ you may have noticed that after a sorting operation it can happen that all the indexes become shuffled.\n",
    "\n",
    "Wrong indexing can mess up your visualizations sometimes.\n",
    "\n",
    "The point is: in certain cases, when you have done a transformation on your dataframe, you have to re-index the rows. For that, you can use the `reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values(by=['water_need'], ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our new dataframe kept the old indexes, too. \n",
    "If you want to remove them, just add the `drop = True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.sort_values(by=['water_need'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fillna\n",
    "\n",
    "Let‚Äôs rerun the left-merge method that we have used above. The `NaN` values appearing in lions rows may be something that we want to discard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that we have NaN values for lions.\n",
    "`NaN` usually is a problem when we do our data analysis, so one can choose to replace those values with something more meaningful.\n",
    "Usually to get unbiased results, we substitute the NaN with the mean for that variable.\n",
    "In some other case we could put a default value, this can be a $0$ value, or in other cases a specific string value. \n",
    "\n",
    "In this case even if the `zoo_eat` dataframe gave us no clue about the lion, we will put our best guess.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "‚ö†Ô∏è Of course you never add best guesses to your real data, unless you are 100% you are not biasing your results\n",
    "</div>\n",
    "\n",
    "Let‚Äôs use the `fillna` method, which basically finds and replaces all `NaN` values in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo.merge(zoo_eats, how='left').fillna('meat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A data analysis\n",
    "\n",
    "Furthermore we are going to use pandas tools to perform an _exploratory data analysis_ over a dataset.\n",
    "\n",
    "### Import data\n",
    "\n",
    "An interesting feature of read methods in pandas are that it is allowed to give them a url and they will read data from it.\n",
    "\n",
    "#### The dataset\n",
    "\n",
    "We are going to use a famous dataset, the notorious [IMBD movies dataset](MISSING LINK).\n",
    "The IMDB movie reviews dataset is a set of reviews, there are various versions of it, one can read more about the version used in these lectures in the [official documentation](https://files.grouplens.org/datasets/movielens/ml-latest-README.html). \n",
    "The dataset is available online and can be either directly downloaded from Stanford‚Äôs website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "df_movies = pd.read_csv(\"https://raw.githubusercontent.com/LearnDataSci/articles/master/Python%20Pandas%20Tutorial%20A%20Complete%20Introduction%20for%20Beginners/IMDB-Movie-Data.csv\", index_col=\"Title\")\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can check the dimensions by a method inherited by numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime you might have data with duplicated rows (for example if you are writing to your saving file at higher frequency to which you query experimental variables (e.g. keyboard)). In that case you might need to use the `drop_duplicates` method to clear duplicated rows. The function works by checking the content of each row with subsequent one and find a match only when all columns are identical. You will also return a copy of your `DataFrame`.\n",
    "\n",
    "Another important argument for `drop_duplicates` is `keep`, which has three possible options:\n",
    "\n",
    "1. `\"first\"`: (default) Drop duplicates except for the first occurrence.\n",
    "2. `\"last\"`: Drop duplicates except for the last occurrence.\n",
    "3. `False`: Drop all duplicates.\n",
    "\n",
    "Since we did not indicate the keep argument in the previous example it was defaulted to `\"first\"`. This means that if two rows are the same pandas will drop the second row and keep the first one. \n",
    "Using `last` has the opposite effect: the first row is dropped.\n",
    "\n",
    "`keep = False`, on the other hand, will drop all duplicates. If two rows are the same then both will be dropped. \n",
    "Let's see what happens to `temp_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat([df_movies,df_movies])\n",
    "# duplicate the entire dataset and append it \n",
    "temp_df.drop_duplicates(inplace=True, keep=False)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all rows were duplicates, `keep=False` dropped them all resulting in zero rows being left over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column clean up\n",
    "\n",
    "Sometimes you will work on other people collected datasets, these datasets might contain verbose column names with symbols, upper and lowercase words, spaces, and typos.\n",
    "Unless you have a clear instruction of the experiment, how the dataset was collected and what were the exact name of the variables it makes sense to spend a little time cleaning up column names.\n",
    "Also for how pandas wok with indexing it may be beneficial to change some of the column names.\n",
    "\n",
    "Here is how to print the column names of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does `columns` come in handy if you want to rename columns by allowing for simple copy and paste, it is also useful if you need to understand why you are receiving a Key Error when selecting data by column.\n",
    "\n",
    "We can use the `rename` method to rename certain or all columns via a dict. We do not want parentheses, so let's rename those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = df_movies.rename(columns={\n",
    "    'Runtime (Minutes)': 'Runtime',\n",
    "    'Revenue (Millions)': 'Revenue_millions'\n",
    "})\n",
    "df_movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. But what if we want to lowercase all names? \n",
    "Instead of using `rename` we could also reassing the column attribute to a list of names like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.columns = [col.lower() for col in df_movies]\n",
    "df_movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "\n",
    "When exploring data, one most likely encounters missing or null values, which are essentially placeholders for non-existent values. \n",
    "Most commonly in these contexts one faces Python's `None` or NumPy's `np.nan`, each of which have to be handled differently according to situations.\n",
    "\n",
    "There are two options in dealing with nulls:\n",
    "\n",
    "1. Get rid of rows or columns with nulls\n",
    "2. Replace nulls with non-null values\n",
    "\n",
    "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our DataFrame are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice `isnull` returns a DataFrame where each cell is either `True` or `False` depending on that cell's null status.\n",
    "\n",
    "To count the number of nulls we can use the function any that returns a `Series` of True/False if any of the elements in a single row is True after teh `isnull` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_null_rows = df_movies.isnull().any(axis=1).sum()\n",
    "print(\"Number of rows with at least one null value:\", num_null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing nulls is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.dropna() # This drops the whole line where a NaN appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will delete any row with at least a single null value, but it will return a new DataFrame without altering the original one.\n",
    "\n",
    "Other than just dropping rows, you can also drop columns with null values by setting `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, this operation would drop the `revenue_millions` and `metascore` columns.\n",
    "\n",
    "##### Imputing\n",
    "Imputation is referred to keeping valuable data replacing the null values.\n",
    "\n",
    "There may be instances where dropping every row with a null value removes too big a chunk from your dataset, so instead we can impute that null with another value, usually the mean or the median of that column.\n",
    "\n",
    "Let's look at imputing the missing values in the `revenue_millions` column. First we will extract that column series into its own variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenues = df_movies['revenue_millions']\n",
    "revenues.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly different formatting than a DataFrame, but we still have our Title index.\n",
    "\n",
    "We will impute the missing values of revenue using the mean.\n",
    "\n",
    "__Quick Exercise__: Find the mean of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenues_mean = revenues.mean()\n",
    "print(\"Mean revenue:\", revenues_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to replace NaN values with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenues = revenues.fillna(revenues_mean)\n",
    "df_movies['revenue_millions'] = revenues\n",
    "df_movies['revenue_millions'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Applying functions\n",
    "\n",
    "Operating on the datasets means that sometimes we will also be required to do more complicated operations on our dataframes entries is ubiquitous. \n",
    "The iteration over a DataFrame or Series as you would with a list is possible, however, because of the complex structure of a dataframe, this is really not efficient.\n",
    "\n",
    "An efficient alternative is to `apply` a function to the dataset.\n",
    "For example, we could use a function to convert movies with an 8.0 or greater to a string value of `\"good\"` and the rest to `\"bad\"` and use this transformed values to create a new column.\n",
    "\n",
    "First we would create a function that, when given a rating, determines if it's good or bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_function(x, value):\n",
    "    if x >= value:\n",
    "        return \"good\"\n",
    "    else:\n",
    "        return \"bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to send the entire rating column through this function, which is what `apply` does.\n",
    "This operation is _vectorised_ so taking advantage of the pandas structures, it is more efficient than an explicit for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies[\"rating_category\"] = df_movies[\"rating\"].apply(rating_function, value = 8.0)\n",
    "df_movies.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` method passes every value in the rating column through the `rating_function` and then returns a new Series.\n",
    "This Series is then assigned to a new column called `rating_category`.\n",
    "\n",
    "You can also use anonymous functions as well. This `lambda` function achieves the same result as `rating_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lambada = lambda x: 'good' if x >= 8.0 else 'bad' # check if you can input to the lambda expression\n",
    "\n",
    "df_movies[\"rating_category\"] = df_movies[\"rating\"].apply(\n",
    "    my_lambada)\n",
    "df_movies.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas pipe\n",
    "\n",
    "When working with data, we often need to apply multiple transformations sequentially‚Äîfor instance, for example first cleaning the data (removing null values), then filtering rows based on certain criteria. The [`pipe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html) method provides an elegant way to chain these transformations together.\n",
    "\n",
    "Instead of nesting function calls or using intermediate variables, `pipe` lets you write transformations as a readable pipeline:\n",
    "\n",
    "```python\n",
    "df_movies.pipe(remove_null).pipe(drop_genre, \"Comedy\")\n",
    "```\n",
    "\n",
    "Each function in the pipeline must:\n",
    "1. Accept a DataFrame as its **first parameter**\n",
    "2. **Return** a DataFrame (which becomes the input for the next step)\n",
    "\n",
    "Additional arguments can be passed after the function name (e.g., `\"Comedy\"` is passed to `drop_genre`).\n",
    "\n",
    "Let's define two transformation functions to see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def remove_null(df_movies):\n",
    "    \"\"\"remove null values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        the base dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        the dataframe with null values removed\n",
    "    \"\"\"\n",
    "    return df_movies.dropna()\n",
    "\n",
    "def drop_genre(df_movies, genre):\n",
    "    \"\"\"remove a specific genre movies\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        the base dataframe\n",
    "    \n",
    "    genre: str\n",
    "        the movie genre to be removed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        the filtered dataframe \n",
    "    \"\"\"\n",
    "    series_remove = df_movies.genre.str.contains(f\"{genre}\") # Boolean series\n",
    "    idx_to_drop = series_remove[series_remove].index # Find indices in the series and use them to drop rows from the dataframe\n",
    "    print(f\"Removing {len(idx_to_drop)} rows\")\n",
    "    print(idx_to_drop)\n",
    "    return df_movies.drop(index=idx_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily build a pipeline over a dataframe by calling `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_movies.pipe(remove_null)\n",
    "    .pipe(drop_genre, \"Comedy\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICN_Programming_Course-E5uewI15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
